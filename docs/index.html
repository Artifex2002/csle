---
layout: default
---

<div class="container">
    <div class="jumbotron bg-white text-center">
        <div class="container logoIndex">
            <img src="img/csle_logo_cropped.png" width="45%">
            <p class="lead ">A framework for building self-learning security systems.
            </p>
        </div>
    </div>


    <div id="exTab2" class="container indexPageContainer">

        <h2 class="display text-left">Installation</h2>
        <p>
            CSLE is available for download on <a href="https://github.com/Limmen/csle">Github</a>
            The python APIs are available on <a href="https://pypi.org/project/csle-collector/">PyPi</a>
            and can be installed using Pip. The Docker containers are available on
            <a href="https://pypi.org/project/csle-collector/"> Docker hub </a> and can be installed using the
            Docker command-line tool. The management system can be installed using the local files in the Git
            repository.
        </p>
        <ul class="nav nav-tabs">
            <li class="active">
                <a href="#1" data-toggle="tab">Pip</a>
            </li>
            <li><a href="#2" data-toggle="tab">Docker</a>
            </li>
        </ul>
        <div class="tab-content" id="installation">
            <div class="tab-pane active" id="1">
                {% highlight bash %}
                pip install csle-collector \
                csle-common \
                csle-attacker \
                csle-defender \
                gym-csle-stopping-game \
                csle-system-identification \
                csle-agents \
                csle-ryu \
                csle-rest-api
                {% endhighlight %}
            </div>
            <div class="tab-pane" id="2">
                {% highlight bash %}
                TODO
                {% endhighlight %}
            </div>

        </div>


        <hr>
        <h2 class="display text-left">Usage</h2>
        <p>The example below demonstrates.. </p>
        <p>You can find additional examples in the <a href="/docs/">documentation </a> and a video demonstration is
            available on <a href="https://www.youtube.com/watch?v=18P7MjPKNDg&t=1s">Youtube</a>.</p>
        <ul class="nav nav-tabs">
            <li class="active">
                <a href="#4" data-toggle="tab">Emulate a network environment</a>
            </li>
            <li><a href="#5" data-toggle="tab">Emulate a network attack</a>
            </li>
            <li><a href="#6" data-toggle="tab">Learn a system model</a>
            </li>
            <li><a href="#7" data-toggle="tab">Learn a security policy</a>
            </li>
            <li>
                <a href="#8" data-toggle="tab">Manage emulations & learning processes</a>
            </li>
        </ul>
        <div id="usage" class="tab-content ">
            <div class="tab-pane active" id="4">
                {% highlight java %}
                // Just a bit of code, how bad could it be?
                int i=1;
                while (true) {
                System.out.println(i);
                if (i > 99) break;
                i++;
                }
                {% endhighlight %}
            </div>
            <div class="tab-pane" id="5">
                {% highlight java %}
                // As it turns out, very.
                AST ast = AST.newAST(AST.JLS8);
                Block block = ast.newBlock();
                VariableDeclarationFragment fragment = ast.newVariableDeclarationFragment();
                fragment.setName(ast.newSimpleName("i"));
                fragment.setInitializer(ast.newNumberLiteral("1"));
                VariableDeclarationStatement statement = ast.newVariableDeclarationStatement(fragment);
                statement.setType(ast.newPrimitiveType(PrimitiveType.INT));
                block.statements().add(statement);
                WhileStatement whileStatement = ast.newWhileStatement();
                whileStatement.setExpression(ast.newBooleanLiteral(true));
                Block whileBody = ast.newBlock();
                MethodInvocation methodInvocation = ast.newMethodInvocation();
                methodInvocation.setName(ast.newSimpleName("println"));
                methodInvocation.setExpression(ast.newName("System.out"));
                methodInvocation
                .arguments()
                .add(ast.newSimpleName("i"));
                whileBody
                .statements()
                .add(ast.newExpressionStatement(methodInvocation));
                IfStatement ifStatement = ast.newIfStatement();
                InfixExpression infixExpression = ast.newInfixExpression();
                infixExpression.setLeftOperand(ast.newSimpleName("i"));
                infixExpression.setOperator(Operator.GREATER);
                infixExpression.setRightOperand(ast.newNumberLiteral("99"));
                ifStatement.setExpression(infixExpression);
                ifStatement.setThenStatement(ast.newBreakStatement());
                whileBody
                .statements()
                .add(ifStatement);
                PostfixExpression postfixExpression = ast.newPostfixExpression();
                postfixExpression.setOperator(PostfixExpression.Operator.INCREMENT);
                postfixExpression.setOperand(ast.newSimpleName("i"));
                whileBody
                .statements()
                .add(ast.newExpressionStatement(postfixExpression));
                whileStatement.setBody(whileBody);
                block.statements().add(whileStatement);
                {% endhighlight %}
            </div>

            <div class="tab-pane" id="6">
                {% highlight java %}
                // That's certainly better!
                FluentStatement variableDeclaration = stmnt(decl("int",1));
                FluentStatement methodInvocation = stmnt(invocation(name("System.out"),
                new ArrayList
                <FluentType>(),
                    "println",
                    n("i")));
                    FluentBlock whileBody = block(methodInvocation,
                    if_(infix(">")
                    .left(n("i"))
                    .right(i(99))).then(break_()),
                    stmnt(postfix("++").operand(n("i"))));
                    block(variableDeclaration,
                    while_(b(true))
                    .do_(whileBody))
                    .build();
                    {% endhighlight %}
            </div>

            <div class="tab-pane" id="7">
                {% highlight python %}
                import csle_common.constants.constants as constants
                from csle_common.dao.training.experiment_config import ExperimentConfig
                from csle_common.metastore.metastore_facade import MetastoreFacade
                from csle_common.dao.training.agent_type import AgentType
                from csle_common.dao.training.hparam import HParam
                from csle_common.dao.training.player_type import PlayerType
                from csle_agents.agents.t_spsa.t_spsa_agent import TSPSAAgent
                import csle_agents.constants.constants as agents_constants
                from gym_csle_stopping_game.util.stopping_game_util import StoppingGameUtil

                emulation_env_config = MetastoreFacade.get_emulation_by_name("csle-level9-001")
                simulation_env_config = MetastoreFacade.get_simulation_by_name("csle-stopping-pomdp-defender-001")
                experiment_config = ExperimentConfig(
                output_dir = f"{constants.LOGGING.DEFAULT_LOG_DIR}tspsa_test",
                title="T-SPSA test",
                random_seeds = [399, 98912,999,555],
                agent_type = AgentType.T_SPSA,
                log_every = 1,
                hparams = {
                agents_constants.T_SPSA.N: HParam(value=200, name=agents_constants.T_SPSA.N,
                descr="the number of training iterations"),
                agents_constants.T_SPSA.c: HParam(value=10, name=agents_constants.T_SPSA.c,
                descr="scalar coefficient for determining perturbation sizes in T-SPSA"),
                agents_constants.T_SPSA.a: HParam(value=1, name=agents_constants.T_SPSA.a,
                descr="scalar coefficient for determining gradient step sizes in T-SPSA"),
                agents_constants.T_SPSA.A: HParam(value=100, name=agents_constants.T_SPSA.A,
                descr="scalar coefficient for determining gradient step sizes in T-SPSA"),
                agents_constants.T_SPSA.LAMBDA: HParam(value=0.602, name=agents_constants.T_SPSA.LAMBDA,
                descr="scalar coefficient for determining perturbation sizes in T-SPSA"),
                agents_constants.T_SPSA.EPSILON: HParam(value=0.101, name=agents_constants.T_SPSA.EPSILON,
                descr="scalar coefficient for determining gradient step sizes in T-SPSA"),
                agents_constants.T_SPSA.L: HParam(value=3, name="L", descr="the number of stop actions"),
                agents_constants.COMMON.EVAL_BATCH_SIZE: HParam(value=10, name=agents_constants.COMMON.EVAL_BATCH_SIZE,
                descr="number of iterations to evaluate theta"),
                agents_constants.T_SPSA.THETA1: HParam(value=[-4,-4,-4], name=agents_constants.T_SPSA.THETA1,
                descr="initial thresholds"),
                agents_constants.COMMON.SAVE_EVERY: HParam(value=1000, name=agents_constants.COMMON.SAVE_EVERY,
                descr="how frequently to save the model"),
                agents_constants.COMMON.CONFIDENCE_INTERVAL: HParam(value=0.95,
                name=agents_constants.COMMON.CONFIDENCE_INTERVAL,
                descr="confidence interval"),
                agents_constants.COMMON.MAX_ENV_STEPS: HParam(value=500, name=agents_constants.COMMON.MAX_ENV_STEPS,
                descr="maximum number of steps in the environment"),
                agents_constants.T_SPSA.GRADIENT_BATCH_SIZE: HParam(value=1,
                name=agents_constants.T_SPSA.GRADIENT_BATCH_SIZE,
                descr="the batch size of the gradient estimator"),
                agents_constants.COMMON.RUNNING_AVERAGE: HParam(value=100, name=agents_constants.COMMON.RUNNING_AVERAGE,
                descr="the number of samples to include when computing the running avg")
                },
                player_type=PlayerType.DEFENDER, player_idx=0)
                agent = TSPSAAgent(emulation_env_config=emulation_env_config,
                simulation_env_config=simulation_env_config,
                experiment_config=experiment_config)
                simulation_env_config.simulation_env_input_config.stopping_game_config.R =
                list(StoppingGameUtil.reward_tensor(
                R_INT=-1, R_COST=-2, R_SLA=0, R_ST=2, L=3))
                experiment_execution = agent.train()
                MetastoreFacade.save_experiment_execution(experiment_execution)
                for policy in experiment_execution.result.policies.values():
                MetastoreFacade.save_multi_threshold_stopping_policy(multi_threshold_stopping_policy=policy)
                {% endhighlight %}
            </div>

            <div class="tab-pane" id="8">
                TODO
            </div>
        </div>
    </div>


    <hr>
    <div class="container">

        <h2 class="display text-left">Why CSLE?</h2>
        <p>
            An organization’s security strategy has traditionally been defined, implemented, and updated by domain experts.
            Although this approach can provide basic security for an organization’s communication and computing infrastructure,
            a growing concern is that infrastructure update cycles become shorter and attacks increase in sophistication.
            Consequently, the security requirements become increasingly difficult to meet. To address this challenge, significant efforts have
            started to automate security frameworks and the process of obtaining effective security policies.
        </p>
        <p>
            Reinforcement learning has emerged as a promising approach to approximate optimal control
            strategies in non-trivial scenarios, and fundamental breakthroughs demonstrated by systems like AlphaGo
            and OpenAI Five have inspired us and other researchers to study reinforcement learning with the goal
            to automate security functions. While several encouraging results have been obtained following this
            approach, key challenges remain.
            The main challenge is to narrow the gap between the environment where strategies are evaluated and
            a scenario playing out in a real system. Most of the results obtained so far are limited to abstract
            simulation environments and it is not clear how the results transfer to practical infrastructures. To address this
            challenge, we developed CSLE, which is a framework that allows using reinforcement learning to optimize security
            policies based on real system data and allows to evaluate learned strategies in practical IT infrastructures.
        </p>
        <p>
            Recently, several efforts have started to build similar frameworks as CSLE with the goal to automate
            network security functions through reinforcement learning (see
            <a href="https://github.com/Limmen/awesome-rl-for-cybersecurity">survey</a>). Most notably, there is
            <a href="https://github.com/microsoft/CyberBattleSim">CyberBattleSim</a> by Microsoft,
            <a href="https://github.com/cage-challenge/CybORG">CyBorg</a> by
            the Australian department of defence, <a href="https://github.com/dstl/YAWNING-TITAN">Yawning Titan</a> by
            the UK Defence Science and Technology Laboratory (DSTL), and
            <a href="https://arxiv.org/pdf/2103.07583.pdf">FARLAND</a>
            developed at USA's National Security Agency (NSA). Some of these frameworks only include
            simulation components and some of them include both simulation and emulation components.

            In contrast to these frameworks, CSLE includes both a simulation component and an emulation component,
            and has demonstrated the capabilitiy to learn near-optimal defender strategies on specific use cases
            (see <a href="publications">publications</a>).
        </p>
    </div>

    <hr>
    <div class="row FeaturesRow">
        <div class="col-sm-4">
            <h1 class="text-center">
                <img src="img/arch.png" class="onlineLearningLogo" width="90%">
            </h1>
            <h3 class="text-center learningText">Automated Security</h3>
            <p>
                The CSLE framework for automating the process of obtaining effective security policies includes three
                parts:
                (1) an emulation system that emulates the target system and is used to evaluate defender strategies
                obtained from self-play;
                (2) a system identification method that produces a model of the target system based on measurements from
                the digital twin;
                and (3) a simulation environment that efficiently executes the self-play process using reinforcement
                learning
                techniques to obtain near optimal defender strategies.
            </p>
        </div>
        <div class="col-sm-4">
            <h1 class="text-center">
                <img src="img/emulation.png" class="emulationLogo" width="35%">
            </h1>
            <h3 class="text-center emulationText">Emulation System</h3>
            <p>
                The emulation system allows emulating large scale IT infrastructures and network traffic,
                i.e client traffic, cyber attacks, and automated defenses.
                It executes on a cluster of machines that runs a virtualization layer provided by Docker containers
                and virtual links. It implements network isolation and traffic shaping on the containers using
                network namespaces and the NetEm module in the Linux kernel. Resource constraints of the containers,
                e.g. CPU and memory constraints, are enforced using cgroups.
                <!--                Have a look at the <a href="https://github.com/Limmen/csle/examples">examples</a>.-->
            </p>
        </div>
        <div class="col-sm-4">
            <h1 class="text-center">
                <img src="img/rl.png" class="RlLogo" width="70%">
            </h1>
            <h3 class="text-center RlText">Simulation System</h3>
            <p>
                The simulation system of CSLE allows running reinforcement learning and optimization algorithms
                to learn security strategies. Formally, we model the interaction between an attacker and
                a defender as a Markov game. We then use simulations of self-play where autonomous agents interact and
                continuously update their strategies based on experience from previously played games.
                To automatically update strategies in the game, several methods can be used,
                including computational game theory, dynamic programming, evolutionary algorithms, and reinforcement
                learning.
            </p>
        </div>
    </div>
</div>
